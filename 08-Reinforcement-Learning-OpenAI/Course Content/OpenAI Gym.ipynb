{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Attempting to Import Gym\")\n",
    "print(\"Remember you can use pip install gym at your command line.\")\n",
    "import gym\n",
    "print(gym.__version__)\n",
    "print(\"It worked!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Intro to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gotta import gym!\n",
    "import gym\n",
    "\n",
    "# Make the environment, replace this string with any\n",
    "# from the docs. (Some environments have dependencies)\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Reset the environment to default beginning\n",
    "env.reset()\n",
    "\n",
    "# Using _ as temp placeholder variable\n",
    "for _ in range(1000):\n",
    "    # Render the env\n",
    "    #env.render()\n",
    "\n",
    "    # Still a lot more explanation to come for this line!\n",
    "    env.step(env.action_space.sample()) # take a random action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym Environment Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gotta import gym!\n",
    "import gym\n",
    "\n",
    "# Make the environment, replace this string with any\n",
    "# from the docs. (Some environments have dependencies)\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Reset the environment to default beginning\n",
    "# Default observation variable\n",
    "print(\"Initial Observation\")\n",
    "observation = env.reset()\n",
    "print(observation)\n",
    "\n",
    "print('\\n')\n",
    "for _ in range(1):\n",
    "\n",
    "\n",
    "    # Random Action\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # Get the 4 observation values discussed\n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "    print(\"Performed One Random Action\")\n",
    "    print('\\n')\n",
    "    print('observation')\n",
    "    print(observation)\n",
    "    print('\\n')\n",
    "\n",
    "    print('reward')\n",
    "    print(reward)\n",
    "    print('\\n')\n",
    "\n",
    "    print('done')\n",
    "    print(done)\n",
    "    print('\\n')\n",
    "\n",
    "    print('info')\n",
    "    print(info)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "# print(env.action_space.)\n",
    "# #> Discrete(2)\n",
    "# print(env.observation_space)\n",
    "# #> Box(4,)\n",
    "observation = env.reset()\n",
    "\n",
    "for t in range(1000):\n",
    "\n",
    "    #env.render()\n",
    "\n",
    "    cart_pos , cart_vel , pole_ang , ang_vel = observation\n",
    "\n",
    "    # Move Cart Right if Pole is Falling to the Right\n",
    "\n",
    "    # Angle is measured off straight vertical line\n",
    "    if pole_ang > 0:\n",
    "        # Move Right\n",
    "        action = 1\n",
    "    else:\n",
    "        # Move Left\n",
    "        action = 0\n",
    "\n",
    "    # Perform Action\n",
    "    observation , reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "###############################################\n",
    "######## PART ONE: NETWORK VARIABLES #########\n",
    "#############################################\n",
    "\n",
    "# Observation Space has 4 inputs\n",
    "num_inputs = 4\n",
    "\n",
    "num_hidden = 4\n",
    "\n",
    "# Outputs the probability it should go left\n",
    "num_outputs = 1\n",
    "\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "\n",
    "###############################################\n",
    "######## PART TWO: NETWORK LAYERS #########\n",
    "#############################################\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None,num_inputs])\n",
    "hidden_layer_one = tf.layers.dense(\n",
    "    X,num_hidden,\n",
    "    activation=tf.nn.relu,kernel_initializer=initializer)\n",
    "hidden_layer_two = tf.layers.dense(\n",
    "    hidden_layer_one,\n",
    "    num_hidden,\n",
    "    activation=tf.nn.relu,kernel_initializer=initializer)\n",
    "\n",
    "# Probability to go left\n",
    "output_layer = tf.layers.dense(\n",
    "    hidden_layer_one,\n",
    "    num_outputs,\n",
    "    activation=tf.nn.sigmoid,kernel_initializer=initializer)\n",
    "\n",
    "# [ Prob to go left , Prob to go right]\n",
    "probabilties = tf.concat(\n",
    "    axis=1, values=[output_layer, 1 - output_layer])\n",
    "\n",
    "# Sample 1 randomly based on probabilities\n",
    "action = tf.multinomial(probabilties, num_samples=1)\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "\n",
    "###############################################\n",
    "######## PART THREE: SESSION #########\n",
    "#############################################\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "epi = 50\n",
    "step_limit = 500\n",
    "avg_steps = []\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for i_episode in range(epi):\n",
    "        obs = env.reset()\n",
    "\n",
    "        for step in range(step_limit):\n",
    "            # env.render()\n",
    "            action_val = action.eval(\n",
    "                feed_dict={X: obs.reshape(1, num_inputs)})\n",
    "            obs, reward, done, info = env.step(action_val[0][0])\n",
    "            if done:\n",
    "                avg_steps.append(step)\n",
    "                print('Done after {} steps'.format(step))\n",
    "                break\n",
    "print(\"After {} episodes the average cart steps before done was {}\".format(\n",
    "    epi,np.mean(avg_steps)))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "##########################\n",
    "### VARIABLES ###########\n",
    "########################\n",
    "\n",
    "num_inputs = 4\n",
    "num_hidden = 4\n",
    "num_outputs = 1\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "\n",
    "#################################\n",
    "### CREATING THE NETWORK #######\n",
    "###############################\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, num_inputs])\n",
    "\n",
    "hidden_layer = tf.layers.dense(\n",
    "    X,\n",
    "    num_hidden, \n",
    "    activation=tf.nn.elu, \n",
    "    kernel_initializer=initializer)\n",
    "logits = tf.layers.dense(hidden_layer, num_outputs)\n",
    "outputs = tf.nn.sigmoid(logits)  # probability of action 0 (left)\n",
    "\n",
    "probabilties = tf.concat(axis=1, values=[outputs, 1 - outputs])\n",
    "action = tf.multinomial( probabilties, num_samples=1)\n",
    "\n",
    "# Convert from Tensor to number for network training\n",
    "y = 1. - tf.to_float(action)\n",
    "\n",
    "########################################\n",
    "### LOSS FUNCTION AND OPTIMIZATION ####\n",
    "######################################\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    labels=y, logits=logits)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "# https://stackoverflow.com/questions/41954198/\n",
    "# optimizer-compute-gradients-how-the-gradients-are-calculated-programatically\n",
    "# https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n",
    "\n",
    "\n",
    "################################\n",
    "#### GRADIENTS ################\n",
    "##############################\n",
    "gradients_and_variables = optimizer.compute_gradients(cross_entropy)\n",
    "\n",
    "\n",
    "\n",
    "gradients = []\n",
    "gradient_placeholders = []\n",
    "grads_and_vars_feed = []\n",
    "\n",
    "for gradient, variable in gradients_and_variables:\n",
    "    gradients.append(gradient)\n",
    "    gradient_placeholder = tf.placeholder(\n",
    "        tf.float32, shape=gradient.get_shape())\n",
    "    gradient_placeholders.append(gradient_placeholder)\n",
    "    grads_and_vars_feed.append((gradient_placeholder, variable))\n",
    "\n",
    "\n",
    "training_op = optimizer.apply_gradients(grads_and_vars_feed)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "######################################\n",
    "#### REWARD FUNCTIONs ################\n",
    "####################################\n",
    "# CHECK OUT: https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724\n",
    "\n",
    "def helper_discount_rewards(rewards, discount_rate):\n",
    "    '''\n",
    "    Takes in rewards and applies discount rate\n",
    "    '''\n",
    "    discounted_rewards = np.zeros(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    '''\n",
    "    Takes in all rewards, applies helper_discount function and then normalizes\n",
    "    using mean and std.\n",
    "    '''\n",
    "    all_discounted_rewards = []\n",
    "    for rewards in all_rewards:\n",
    "        all_discounted_rewards.append(helper_discount_rewards(rewards,discount_rate))\n",
    "\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean)/reward_std for \n",
    "            discounted_rewards in all_discounted_rewards]\n",
    "\n",
    "########################################\n",
    "#### TRAINING SESSION #################\n",
    "######################################\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "num_game_rounds = 10\n",
    "max_game_steps = 1000\n",
    "num_iterations = 250\n",
    "discount_rate = 0.95\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        print(\"Currently on Iteration: {} \\n\".format(iteration) )\n",
    "\n",
    "        all_rewards = []\n",
    "        all_gradients = []\n",
    "\n",
    "        # Play n amount of game rounds\n",
    "        for game in range(num_game_rounds):\n",
    "\n",
    "            current_rewards = []\n",
    "            current_gradients = []\n",
    "\n",
    "            observations = env.reset()\n",
    "\n",
    "            # Only allow n amount of steps in game\n",
    "            for step in range(max_game_steps):\n",
    "\n",
    "                # Get Actions and Gradients\n",
    "                action_val, gradients_val = sess.run(\n",
    "                    [action,gradients],\n",
    "                    feed_dict={X: observations.reshape(1, num_inputs)})\n",
    "\n",
    "                # Perform Action\n",
    "                observations, reward, done, info = env.step(action_val[0][0])\n",
    "\n",
    "                # Get Current Rewards and Gradients\n",
    "                current_rewards.append(reward)\n",
    "                current_gradients.append(gradients_val)\n",
    "\n",
    "                if done:\n",
    "                    # Game Ended\n",
    "                    break\n",
    "\n",
    "            # Append to list of all rewards\n",
    "            all_rewards.append(current_rewards)\n",
    "            all_gradients.append(current_gradients)\n",
    "\n",
    "        all_rewards = discount_and_normalize_rewards(all_rewards,discount_rate)\n",
    "        feed_dict = {}\n",
    "\n",
    "\n",
    "        for var_index, gradient_placeholder in enumerate(gradient_placeholders):\n",
    "            mean_gradients = np.mean([reward * all_gradients[game_index][step][var_index]\n",
    "                                      for game_index, rewards in enumerate(all_rewards)\n",
    "                                          for step, reward in enumerate(rewards)], axis=0)\n",
    "            feed_dict[gradient_placeholder] = mean_gradients\n",
    "\n",
    "        sess.run(training_op, feed_dict=feed_dict)\n",
    "\n",
    "    print('SAVING GRAPH AND SESSION')\n",
    "    meta_graph_def = tf.train.export_meta_graph(filename='/models/my-650-step-model.meta')\n",
    "    saver.save(sess, '/models/my-650-step-model')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################################\n",
    "### RUN TRAINED MODEL ON ENVIRONMENT #######\n",
    "###########################################\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "observations = env.reset()\n",
    "with tf.Session() as sess:\n",
    "    # https://www.tensorflow.org/api_guides/python/meta_graph\n",
    "    new_saver = tf.train.import_meta_graph('/models/my-650-step-model.meta')\n",
    "    new_saver.restore(sess,'/models/my-650-step-model')\n",
    "\n",
    "    for x in range(500):\n",
    "        env.render()\n",
    "        action_val, gradients_val = sess.run(\n",
    "            [action, gradients], \n",
    "            feed_dict={X: observations.reshape(1, num_inputs)})\n",
    "        observations, reward, done, info = env.step(action_val[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Same as 05 file, just no training, just loads the pre-trained model.\n",
    "\n",
    "\n",
    "##########################\n",
    "### VARIABLES ###########\n",
    "########################\n",
    "\n",
    "num_inputs = 4\n",
    "num_hidden = 4\n",
    "num_outputs = 1\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "\n",
    "#################################\n",
    "### CREATING THE NETWORK #######\n",
    "###############################\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, num_inputs])\n",
    "\n",
    "hidden_layer = tf.layers.dense(\n",
    "    X, \n",
    "    num_hidden, \n",
    "    activation=tf.nn.elu, \n",
    "    kernel_initializer=initializer)\n",
    "logits = tf.layers.dense(hidden_layer, num_outputs)\n",
    "outputs = tf.nn.sigmoid(logits)  # probability of action 0 (left)\n",
    "\n",
    "\n",
    "probabilties = tf.concat(axis=1, values=[outputs, 1 - outputs])\n",
    "action = tf.multinomial( probabilties, num_samples=1)\n",
    "\n",
    "# Convert from Tensor to number for network training\n",
    "y = 1. - tf.to_float(action)\n",
    "\n",
    "########################################\n",
    "### LOSS FUNCTION AND OPTIMIZATION ####\n",
    "######################################\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "# https://stackoverflow.com/questions/41954198/\n",
    "#optimizer-compute-gradients-how-the-gradients-are-calculated-programatically\n",
    "# https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n",
    "\n",
    "\n",
    "################################\n",
    "#### GRADIENTS ################\n",
    "##############################\n",
    "gradients_and_variables = optimizer.compute_gradients(cross_entropy)\n",
    "\n",
    "\n",
    "\n",
    "gradients = []\n",
    "gradient_placeholders = []\n",
    "grads_and_vars_feed = []\n",
    "\n",
    "for gradient, variable in gradients_and_variables:\n",
    "    gradients.append(gradient)\n",
    "    gradient_placeholder = tf.placeholder(tf.float32, shape=gradient.get_shape())\n",
    "    gradient_placeholders.append(gradient_placeholder)\n",
    "    grads_and_vars_feed.append((gradient_placeholder, variable))\n",
    "\n",
    "\n",
    "training_op = optimizer.apply_gradients(grads_and_vars_feed)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "######################################\n",
    "#### REWARD FUNCTIONs ################\n",
    "####################################\n",
    "# CHECK OUT: https://medium.com/@awjuliani/\n",
    "#super-simple-reinforcement-learning-tutorial-part-2-ded33892c724\n",
    "\n",
    "def helper_discount_rewards(rewards, discount_rate):\n",
    "    '''\n",
    "    Takes in rewards and applies discount rate\n",
    "    '''\n",
    "    discounted_rewards = np.zeros(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    return discounted_rewards\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    '''\n",
    "    Takes in all rewards, applies helper_discount function and then normalizes\n",
    "    using mean and std.\n",
    "    '''\n",
    "    all_discounted_rewards = []\n",
    "    for rewards in all_rewards:\n",
    "        all_discounted_rewards.append(helper_discount_rewards(rewards,discount_rate))\n",
    "\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean)/reward_std for \n",
    "            discounted_rewards in all_discounted_rewards]\n",
    "\n",
    "#############################################\n",
    "### RUN TRAINED MODEL ON ENVIRONMENT #######\n",
    "###########################################\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "observations = env.reset()\n",
    "with tf.Session() as sess:\n",
    "    # https://www.tensorflow.org/api_guides/python/meta_graph\n",
    "    new_saver = tf.train.import_meta_graph('/models/my-650-step-model.meta')\n",
    "    new_saver.restore(sess,'/models/my-650-step-model')\n",
    "\n",
    "    for x in range(500):\n",
    "        env.render()\n",
    "        action_val, gradients_val = sess.run(\n",
    "            [action, gradients],\n",
    "            feed_dict={X: observations.reshape(1, num_inputs)})\n",
    "        observations, reward, done, info = env.step(action_val[0][0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
